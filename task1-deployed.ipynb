{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7237546,"sourceType":"datasetVersion","datasetId":4191429},{"sourceId":7237794,"sourceType":"datasetVersion","datasetId":4153541},{"sourceId":7243330,"sourceType":"datasetVersion","datasetId":4172681},{"sourceId":7255519,"sourceType":"datasetVersion","datasetId":4204338},{"sourceId":7255618,"sourceType":"datasetVersion","datasetId":4195934}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install watchdog\n!pip install --no-dependencies --quiet streamlit\n!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n!unzip -o ngrok-stable-linux-amd64.zip\n!pip install --quiet pyngrok\n!pip install --no-dependencies --quiet protobuf==3.20.*   #==4.21.12\n!pip install --no-dependencies --quiet validators","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:58:45.186881Z","iopub.execute_input":"2024-04-26T17:58:45.187254Z","iopub.status.idle":"2024-04-26T17:59:22.958937Z","shell.execute_reply.started":"2024-04-26T17:58:45.187224Z","shell.execute_reply":"2024-04-26T17:59:22.957579Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting watchdog\n  Downloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl.metadata (37 kB)\nDownloading watchdog-4.0.0-py3-none-manylinux2014_x86_64.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: watchdog\nSuccessfully installed watchdog-4.0.0\n--2024-04-26 17:59:03--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\nResolving bin.equinox.io (bin.equinox.io)... 54.237.133.81, 52.202.168.65, 54.161.241.46, ...\nConnecting to bin.equinox.io (bin.equinox.io)|54.237.133.81|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 13921656 (13M) [application/octet-stream]\nSaving to: 'ngrok-stable-linux-amd64.zip'\n\nngrok-stable-linux- 100%[===================>]  13.28M  16.1MB/s    in 0.8s    \n\n2024-04-26 17:59:04 (16.1 MB/s) - 'ngrok-stable-linux-amd64.zip' saved [13921656/13921656]\n\nArchive:  ngrok-stable-linux-amd64.zip\n  inflating: ngrok                   \n","output_type":"stream"}]},{"cell_type":"code","source":"!ngrok authtoken \"ADD YOUR TOKEN HERE\" ","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:59:22.961346Z","iopub.execute_input":"2024-04-26T17:59:22.961657Z","iopub.status.idle":"2024-04-26T17:59:25.131735Z","shell.execute_reply.started":"2024-04-26T17:59:22.961629Z","shell.execute_reply":"2024-04-26T17:59:25.130527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}]},{"cell_type":"code","source":"%%writefile my_app.py\nimport numpy as np \nimport pandas as pd \nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom catboost import CatBoostRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification\nfrom transformers import DebertaTokenizer, DebertaModel\nfrom transformers import BlipProcessor, BlipForConditionalGeneration\n\nfrom torch.utils.data import DataLoader, Dataset\nimport torch\nimport torch.nn as nn\n\nimport requests\nfrom PIL import Image \nfrom io import BytesIO\nfrom tqdm import tqdm\nfrom IPython.display import Image as IPImage, display\n\nimport streamlit as st\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\n\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\nDistil_bert = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\nDistil_bert.classifier = nn.Sequential(\n    nn.Linear(768, 5),\n    nn.Softmax(dim=1)\n)\nDistil_bert.load_state_dict(torch.load('/kaggle/input/nityam-model-1/log_model_state_dict.pth'))\nDistil_bert.eval()\n\n# Initialize BLIP processor and model\nblip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\nblip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n# Load DeBERTa model and tokenizer\ndeberta_tokenizer = DebertaTokenizer.from_pretrained('microsoft/deberta-base')\ndeberta_model = DebertaModel.from_pretrained('microsoft/deberta-base')\n\n\nclass Test_Dataset(Dataset):\n    def __init__(self, Comments_):\n        self.comments = Comments_.copy()\n        self.comments[\"text\"] = self.comments[\"text\"].map(lambda x: tokenizer(x, padding=\"max_length\", truncation=True, return_tensors=\"pt\"))\n    \n    def __len__(self):\n        return len(self.comments)\n    \n    def __getitem__(self, idx):\n        comment = self.comments.loc[idx, \"text\"]\n        return comment\n\n    \ndef infer(model, Test_DL):\n    # Ensure model is in evaluation mode\n    model.eval()\n    pred = []\n    logs = []\n\n    with torch.no_grad():\n        for comments in Test_DL:\n            masks = comments[\"attention_mask\"].squeeze(1).to(device)\n            input_ids = comments[\"input_ids\"].squeeze(1).to(device)\n\n            # Move model to the same device as input tensors\n            model.to(device)\n\n            # Perform inference\n            output = model(input_ids, attention_mask=masks)\n\n            # Move logits and model components to CPU\n            logits = output.logits.cpu().numpy()  # Move logits to CPU and convert to NumPy\n            model.to('cpu')  # Move the model back to CPU for consistency\n\n            logs.append(logits)\n            pred_class = torch.argmax(output.logits.cpu(), dim=1).item()  # Move prediction to CPU\n            pred.append(pred_class)\n\n    return pred, logs\n\n\ndef fetch_image(image_url):\n\n    response = requests.get(image_url)\n    image = None\n    if(response.status_code == 200):\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n    else:\n        black_image_size = (224, 224)  \n        image = Image.new(\"RGB\", black_image_size, \"black\")\n    return image\n    \n    \ndef predict(final_text):\n    def clean(text):\n        text = text.replace(\"<hyperlink>\",\"\").replace(\"<mention>\",\"\")\n        return text\n    x = final_text\n    x = clean(x)\n\n    text_inputs = [x]\n    columns = ['text']\n    df = pd.DataFrame(text_inputs, columns=columns)\n\n    X_test = df\n    Test_data = Test_Dataset(X_test)\n    Test_Loader = DataLoader(Test_data, shuffle=False)\n    pred,logs = infer(Distil_bert,Test_Loader)\n\n    input_ids = deberta_tokenizer.encode(x, return_tensors='pt')\n    with torch.no_grad():\n        embeddings = deberta_model(input_ids).last_hidden_state\n\n    flattened_embeddings = embeddings.view(-1, embeddings.size(-1))\n    mean_embeddings = torch.mean(flattened_embeddings, dim=0)\n    mean_embeddings_np = mean_embeddings.numpy()\n\n    selected_bucket = pred[0]\n#     model_path = f\"/kaggle/input/regression-models/outputs/model_bucket_{selected_bucket}.joblib\"\n    model_path = f\"/kaggle/input/regression-models/outputs/outputs/model_bucket_{selected_bucket}.joblib\"\n    selected_model = XGBRegressor()\n    selected_model.load_model(model_path)\n    combined_features = mean_embeddings\n\n    features_reshaped = combined_features.view(1, -1).numpy()\n\n    prediction = selected_model.predict(features_reshaped)\n#     k = 15*np.exp(prediction/25)\n#     if(k > 5000):\n#         k = 150*prediction\n    return prediction\n\ndef main():\n    \n    st.title(\"Tweet Like Prediction\")\n\n\n    with st.form(\"user_input_form\"):\n\n        img_url = st.text_input(\"Enter the Image URL:\")\n        if img_url:\n            img = fetch_image(img_url)\n        else:\n            img = None\n            \n        if img is not None:\n            img = img.resize((224, 224))\n            st.image(img, caption=\"Processed Image\", use_column_width=True)\n        else:\n            black_image_size = (224, 224)  \n            img = Image.new(\"RGB\", black_image_size, \"black\")\n\n        tweet_content = st.text_input(\"Enter the Tweet Content: \")\n        inferred_company = st.text_input(\"Enter the Inferred Company: \")\n        date_time = st.text_input(\"Enter the Date and Time (e.g., 2018-01-29 10:51:17): \")\n\n        inputs = blip_processor(img, return_tensors=\"pt\")\n        out = blip_model.generate(**inputs)\n        image_caption = blip_processor.decode(out[0], skip_special_tokens=True)\n\n        final_text = (\n            f\"Following is the information about Twitter post.\"\n            f\"Caption for Image of post: {image_caption}, \"\n            f\"Text content: {tweet_content}, \"\n            f\"Inferred company: {inferred_company}, \"\n            f\"Date and time: {date_time} \"\n        )\n        \n        st.write(final_text)\n\n        submitted = st.form_submit_button(\"Submit\")\n        if submitted:\n            prediction = predict(final_text)\n            st.success(f\"Prediction : {prediction}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:59:25.133548Z","iopub.execute_input":"2024-04-26T17:59:25.133869Z","iopub.status.idle":"2024-04-26T17:59:25.144943Z","shell.execute_reply.started":"2024-04-26T17:59:25.133841Z","shell.execute_reply":"2024-04-26T17:59:25.143967Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Writing my_app.py\n","output_type":"stream"}]},{"cell_type":"code","source":"from pyngrok import ngrok\nimport threading\n\ndef run_ngrok():\n    ngrok_tunnel = ngrok.connect(8501)\n    print(f'Public URL: {ngrok_tunnel.public_url}')\n    ngrok_tunnel.block_until_close()\n\nngrok_thread = threading.Thread(target=run_ngrok)\nngrok_thread.start()","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:59:25.146093Z","iopub.execute_input":"2024-04-26T17:59:25.146374Z","iopub.status.idle":"2024-04-26T17:59:25.197372Z","shell.execute_reply.started":"2024-04-26T17:59:25.146351Z","shell.execute_reply":"2024-04-26T17:59:25.195445Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"!streamlit run --server.port 8501 my_app.py > /dev/null","metadata":{"execution":{"iopub.status.busy":"2024-04-26T17:59:25.199777Z","iopub.execute_input":"2024-04-26T17:59:25.200256Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Exception in thread Thread-5 (run_ngrok):\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/threading.py\", line 953, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/tmp/ipykernel_34/567140438.py\", line 7, in run_ngrok\nAttributeError: 'NgrokTunnel' object has no attribute 'block_until_close'\n","output_type":"stream"},{"name":"stdout","text":"Public URL: https://840b-34-168-232-198.ngrok-free.app\ntokenizer_config.json: 100%|██████████████████| 28.0/28.0 [00:00<00:00, 158kB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 8.14MB/s]\ntokenizer.json: 100%|████████████████████████| 466k/466k [00:00<00:00, 37.0MB/s]\nconfig.json: 100%|█████████████████████████████| 483/483 [00:00<00:00, 3.02MB/s]\nmodel.safetensors: 100%|██████████████████████| 268M/268M [00:00<00:00, 276MB/s]\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n2024-04-26 18:01:21.745258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-26 18:01:21.745364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-26 18:01:21.885297: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\npreprocessor_config.json: 100%|████████████████| 287/287 [00:00<00:00, 1.60MB/s]\ntokenizer_config.json: 100%|███████████████████| 506/506 [00:00<00:00, 1.17MB/s]\nvocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 67.9MB/s]\ntokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 12.8MB/s]\nspecial_tokens_map.json: 100%|██████████████████| 125/125 [00:00<00:00, 544kB/s]\nconfig.json: 100%|█████████████████████████| 4.56k/4.56k [00:00<00:00, 15.3MB/s]\npytorch_model.bin: 100%|██████████████████████| 990M/990M [00:03<00:00, 279MB/s]\ntokenizer_config.json: 100%|██████████████████| 52.0/52.0 [00:00<00:00, 235kB/s]\nvocab.json: 100%|████████████████████████████| 899k/899k [00:00<00:00, 29.1MB/s]\nmerges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 49.9MB/s]\nconfig.json: 100%|█████████████████████████████| 474/474 [00:00<00:00, 2.66MB/s]\npytorch_model.bin: 100%|██████████████████████| 559M/559M [00:02<00:00, 273MB/s]\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}